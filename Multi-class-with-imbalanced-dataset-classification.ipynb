{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebfae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# For reproducibility\n",
    "np.random.seed(1237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5721e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('final_210623_nouveaux labels.xlsx',engine='openpyxl')\n",
    "df=df.head(800)\n",
    "df = df[pd.notnull(df['catégorie'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ad9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Date de création',  'Date de modification','Nom du fichier'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e3000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Messages']=df['Messages'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cffa17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_index = df.Messages\n",
    "label_names = df.catégorie\n",
    "\n",
    "\n",
    "data_tags = [\"Messages\",\"catégorie\"]\n",
    "\n",
    "\n",
    "\n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = df.head(800)\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = 10\n",
    "vocab_size = 3091\n",
    "batch_size = 2\n",
    "num_epochs = 10\n",
    "\n",
    "# lets take 80% data as training and remaining 20% for test.\n",
    "train_size = int(len(data) * .8)\n",
    "\n",
    "train_posts = data['Messages'][:train_size]\n",
    "train_tags = data['catégorie'][:train_size]\n",
    "#train_files_names = data['filename'][:train_size]\n",
    "test_posts = data['Messages'][train_size:]\n",
    "test_tags = data['catégorie'][train_size:]\n",
    "#test_files_names = data['filename'][train_size:]\n",
    "\n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7649c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imb = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16cfbc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((640, 3091), (640, 10))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a782440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 512)               1583104   \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,850,890\n",
      "Trainable params: 1,850,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4284c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 - 3s - loss: 3.7738 - accuracy: 0.4902 - val_loss: 1.2732 - val_accuracy: 0.7344\n",
      "Epoch 2/10\n",
      "256/256 - 3s - loss: 5.3923 - accuracy: 0.5898 - val_loss: 2.0637 - val_accuracy: 0.7344\n",
      "Epoch 3/10\n",
      "256/256 - 3s - loss: 4.2901 - accuracy: 0.6875 - val_loss: 2.4028 - val_accuracy: 0.7188\n",
      "Epoch 4/10\n",
      "256/256 - 3s - loss: 4.5463 - accuracy: 0.7363 - val_loss: 1.8430 - val_accuracy: 0.7109\n",
      "Epoch 5/10\n",
      "256/256 - 3s - loss: 4.0171 - accuracy: 0.7617 - val_loss: 2.0404 - val_accuracy: 0.7578\n",
      "Epoch 6/10\n",
      "256/256 - 3s - loss: 3.2805 - accuracy: 0.7773 - val_loss: 2.6248 - val_accuracy: 0.7422\n",
      "Epoch 7/10\n",
      "256/256 - 3s - loss: 2.3676 - accuracy: 0.8301 - val_loss: 1.8778 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "256/256 - 3s - loss: 3.8768 - accuracy: 0.8105 - val_loss: 1.9345 - val_accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "256/256 - 3s - loss: 2.8421 - accuracy: 0.8184 - val_loss: 1.7772 - val_accuracy: 0.7422\n",
      "Epoch 10/10\n",
      "256/256 - 3s - loss: 4.2756 - accuracy: 0.8262 - val_loss: 2.7019 - val_accuracy: 0.7031\n"
     ]
    }
   ],
   "source": [
    "#num_epochs =10\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0829427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3133 - accuracy: 0.5063\n",
      "Test accuracy: 0.5062500238418579\n"
     ]
    }
   ],
   "source": [
    "#score, acc = model.evaluate(x_test, y_test,batch_size=batch_size,verbose=5)\n",
    "loss , acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f102b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another approach using GRU model, takes longer time\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "\n",
    "tokenizer_obj.fit_on_texts(train_posts) \n",
    "\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_posts])\n",
    "\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "X_train_tokens =  tokenizer_obj.texts_to_sequences(train_posts)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(test_posts)\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['f1_score'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = encoder.classes_\n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    #print(test_files_names.iloc[i])\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test);\n",
    "cnf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(14, 12, forward=True)\n",
    "#fig.align_labels()\n",
    "\n",
    "# fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.asarray(label_names), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "fig.savefig(\"txt_classification-smote\" + str(num_epochs) + \".png\", pad_inches=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec699b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a01432",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(prediction, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501bbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels = np.argmax(y_test, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb54849",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = np.argmax(y_train, axis =1)\n",
    "y_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_score = cohen_kappa_score(y_test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(y_test_labels, predictions)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, predictions)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "print(recall)\n",
    "\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us try some sampling technique to remove class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#Over-sampling: SMOTE\n",
    "#SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, \n",
    "#based on those that already exist. It works randomly picking a point from the minority class and computing \n",
    "#the k-nearest neighbors for this point.The synthetic points are added between the chosen point and its neighbors.\n",
    "#We'll use ratio='minority' to resample the minority class.\n",
    "smote = SMOTE('minority')\n",
    "\n",
    "X_sm, y_sm = smote.fit_resample(x_train, y_train)\n",
    "print(X_sm.shape, y_sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced' ,np.unique(y_train_labels) ,y_train_labels)\n",
    "num_epochs =10\n",
    "batch_size = 128\n",
    "history = model.fit(X_sm, y_sm,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=2,\n",
    "                    class_weight=class_weight,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=2)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fccca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_labels):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], prediction[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), prediction.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_labels)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_labels):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_labels\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(num_labels), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_labels):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                        prediction[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], prediction[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "    prediction.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, prediction,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='b')#,\n",
    "                 #**step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a097b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
